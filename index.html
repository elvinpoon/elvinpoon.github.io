<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Elvinpoon.GitHub.io/papernote1 : paper1">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Elvinpoon.GitHub.io/papernote1</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/elvinpoon">View on GitHub</a>

          <h1 id="project_title">Elvinpoon.GitHub.io/papernote1</h1>
          <h2 id="project_tagline">paper1</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="paper-note-1" class="anchor" href="#paper-note-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Paper note 1</h3>

<p>These challenges are infinite-length, concept-evolution, feature-evolution and concept-drift
Infinite-length
is due to the continuous nature of data. Concept-evolution is due to the new emerging classes. Concept-drift is due to
the drifting concept of the stream and feature-evolution is there because of the changing features.</p>

<p>large amounts of informal, irregular, and abbreviated words, large number
of spelling and grammatical errors, and improper sentence structures and mixed languages
Twitter streams contain large amounts of meaningless messages (Hurlock and
Wilson 2011), polluted content (Lee et al. 2011), and rumors (Castillo et al. 2011), which
negatively affect the performance of the detection algorithms</p>

<p>New event detection of specific type of events could be addressed similar to anomaly detection techniques (Khreich et al. 2009), which rely on modeling normal user behavior and detecting any deviation from this baseline profile. This alternate unsupervised learning approach has been shown effective in detecting local festival events, by learning the nor- mal behavior of users in a given location over some period, and in detecting deviations as possible events</p>

<p>When some event descriptions are known, fil- tering techniques could be used to reduce the amounts of irrelevant messages and make it easier for a human expert to annotate a data set of “reasonable” size. </p>

<p>The labeling procedure usually involves two human annotators with specific domain knowledge. An agreement mea- sure, such as Cohen’s Kappa measure (Carletta 1996), is then used to evaluate the level of interannotator agreement. Ambiguous events with a high level of disagreement are discarded.</p>

<p>Existing event detection algorithms can be broadly classified into two categories: document- pivot methods and feature-pivot methods. The former detects events by clustering doc- uments based on the semantics distance between documents [23], while the latter stud-
ies the distributions of words and discovers events by grouping words together [11].</p>

<p>EDCoW could be viewed as a feature-pivot method. We therefore focus on represen- tative feature-pivot methods here.</p>

<p>To the uninitiated, making such decisions can seem like something of a black art because there are many free parameters in the model.</p>

<p>We found that combining several filters with region sizes close to the optimal single region size can improve performance, but adding region sizes far from the optimal range may hurt performance.</p>

<p>In light of these observations, we believe it ad- visable to first perform a coarse line-search over a single filter region size to find the ‘best’ size for the dataset under consideration, and then explore the combination of several region sizes nearby this single best size, including combining both differ- ent region sizes and copies of the optimal sizes.</p>

<p>In practice, the evidence here suggests perhaps searching over a range of 100 to 600.(feature map)</p>

<p>1-max pooling outperforms all other techniques</p>

<p>non-zero dropout rates can help (though very little) at some points from 0.1 to 0.5, depending on datasets. But imposing an l2 norm constraint generally does not improve performance much (except on Opi), and even adversely effects performance on at least one dataset (CR). </p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
